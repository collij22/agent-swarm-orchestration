#!/usr/bin/env python3
"""
Test Script for Quality Guardian Enhancements
Tests comprehensive validation and completion metrics
"""

import sys
import json
import asyncio
import yaml
from pathlib import Path
import tempfile
import shutil

# Add lib directory to path
sys.path.append(str(Path(__file__).parent / "lib"))

from lib.quality_validation import (
    RequirementValidator, EndpointTester, DockerValidator,
    validate_requirements_tool, test_endpoints_tool,
    validate_docker_tool, generate_completion_report_tool
)
from lib.agent_runtime import AgentContext

async def test_requirement_validation():
    """Test requirement validation against TaskManagerAPI"""
    print("\n=== Testing Requirement Validation ===")
    
    # Load test requirements
    with open("test_requirements.yaml", 'r') as f:
        requirements = yaml.safe_load(f)
    
    # Test with actual TaskManagerAPI directory
    task_api_path = Path("TaskManagerAPI")
    
    if task_api_path.exists():
        validator = RequirementValidator("TaskManagerAPI")
        
        # Create a context with some mock data
        context = AgentContext(
            project_requirements=requirements,
            completed_tasks=["rapid-builder", "database-expert"],
            artifacts={"api_spec": {"endpoints": ["/api/v1/tasks", "/api/v1/auth/login"]}},
            decisions=[],
            current_phase="validation"
        )
        
        # Add some created files to context
        context.add_created_file("rapid-builder", "TaskManagerAPI/backend/main.py", "code", True)
        context.add_created_file("rapid-builder", "TaskManagerAPI/backend/database.py", "code", True)
        context.add_created_file("rapid-builder", "TaskManagerAPI/backend/schemas.py", "code", True)
        
        # Run validation
        report = await validator.validate_requirements(requirements, context)
        
        print(f"‚úÖ Validation Complete:")
        print(f"  - Total Requirements: {report.total_requirements}")
        print(f"  - Completed: {report.completed}")
        print(f"  - Partial: {report.partial}")
        print(f"  - Missing: {report.missing}")
        print(f"  - Failed: {report.failed}")
        print(f"  - Completion: {report.completion_percentage:.1f}%")
        
        if report.critical_issues:
            print(f"\n‚ö†Ô∏è Critical Issues:")
            for issue in report.critical_issues:
                print(f"  - {issue}")
        
        if report.missing_components:
            print(f"\n‚ùå Missing Components:")
            for comp in report.missing_components:
                print(f"  - {comp}")
        
        if report.recommendations:
            print(f"\nüí° Recommendations:")
            for rec in report.recommendations:
                print(f"  - {rec}")
        
        # Save detailed report
        report_path = Path("test_validation_report.json")
        with open(report_path, 'w') as f:
            json.dump(report.to_dict(), f, indent=2)
        print(f"\nüìÑ Detailed report saved to: {report_path}")
        
        # Test the summary generation
        print("\n" + "="*60)
        print(report.get_summary())
        
    else:
        print("‚ùå TaskManagerAPI directory not found - run the workflow first")
        print("   Creating mock project for testing...")
        
        # Create a minimal mock project
        mock_dir = Path(tempfile.mkdtemp(prefix="mock_project_"))
        try:
            # Create some mock files
            (mock_dir / "backend").mkdir()
            (mock_dir / "backend" / "main.py").write_text("# Main API file")
            (mock_dir / "backend" / "database.py").write_text("# Database models")
            (mock_dir / "README.md").write_text("# Test Project")
            (mock_dir / "requirements.txt").write_text("fastapi\nsqlalchemy")
            
            validator = RequirementValidator(str(mock_dir))
            context = AgentContext(
                project_requirements=requirements,
                completed_tasks=[],
                artifacts={},
                decisions=[],
                current_phase="validation"
            )
            
            report = await validator.validate_requirements(requirements, context)
            print(f"‚úÖ Mock validation: {report.completion_percentage:.1f}% complete")
            
        finally:
            shutil.rmtree(mock_dir, ignore_errors=True)

async def test_docker_validation():
    """Test Docker validation capabilities"""
    print("\n=== Testing Docker Validation ===")
    
    # Create a test directory with mock Dockerfile
    test_dir = Path(tempfile.mkdtemp(prefix="docker_test_"))
    
    try:
        # Create a sample Dockerfile
        dockerfile_content = """FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py"]
"""
        (test_dir / "Dockerfile").write_text(dockerfile_content)
        
        # Test validation
        validator = DockerValidator(str(test_dir))
        
        # Validate Dockerfile syntax
        result = await validator.validate_dockerfile()
        print(f"‚úÖ Dockerfile validation:")
        print(f"  - Valid: {result['valid']}")
        if not result['valid'] and 'issues' in result:
            print(f"  - Issues found:")
            for issue in result['issues']:
                print(f"    ‚Ä¢ {issue}")
        
        # Test the tool function
        tool_result = await validate_docker_tool(str(test_dir), test_build=False)
        print(f"\n‚úÖ Docker tool result:")
        print(tool_result)
        
    finally:
        shutil.rmtree(test_dir, ignore_errors=True)

async def test_completion_report():
    """Test completion report generation"""
    print("\n=== Testing Completion Report Generation ===")
    
    # Create a rich context
    context = AgentContext(
        project_requirements={"name": "TestProject", "features": ["API", "Database", "Frontend"]},
        completed_tasks=["requirements-analyst", "project-architect", "rapid-builder", "quality-guardian"],
        artifacts={
            "api_design": {"endpoints": 10, "models": 5},
            "database_schema": {"tables": 5, "relationships": 8}
        },
        decisions=[
            {"decision": "Use FastAPI for backend", "rationale": "High performance"},
            {"decision": "PostgreSQL for database", "rationale": "ACID compliance"},
            {"decision": "React for frontend", "rationale": "Component reusability"}
        ],
        current_phase="validation"
    )
    
    # Add created files
    context.add_created_file("rapid-builder", "backend/main.py", "code", True)
    context.add_created_file("rapid-builder", "backend/database.py", "code", True)
    context.add_created_file("frontend-specialist", "frontend/App.jsx", "code", True)
    context.add_created_file("devops-engineer", "Dockerfile", "config", True)
    context.add_created_file("quality-guardian", "tests/test_api.py", "test", True)
    
    # Add some incomplete tasks
    context.add_incomplete_task("frontend-specialist", "Create user dashboard", "API not ready")
    context.add_incomplete_task("devops-engineer", "Setup CI/CD", "Tests not complete")
    
    # Mark files for verification
    context.add_verification_required("backend/main.py")
    context.add_verification_required("Dockerfile")
    context.add_verification_required("frontend/package.json")
    
    # Generate report
    report = await generate_completion_report_tool(context=context, project_path=".")
    
    print("‚úÖ Completion report generated:")
    print(report)

async def test_tool_integration():
    """Test integration of quality tools with agent runtime"""
    print("\n=== Testing Tool Integration ===")
    
    from lib.agent_runtime import create_quality_tools
    
    # Get quality tools
    tools = create_quality_tools()
    
    print(f"‚úÖ Quality tools available: {len(tools)}")
    for tool in tools:
        print(f"  - {tool.name}: {tool.description}")
        print(f"    Parameters: {list(tool.parameters.keys())}")
    
    # Test that tools can be called
    if tools:
        # Test validate_requirements tool
        test_tool = tools[0]  # validate_requirements
        print(f"\n‚úÖ Testing {test_tool.name} tool...")
        
        # Create minimal requirements
        test_reqs = {
            "features": ["Test API endpoint", "Database connection"],
            "testing_requirements": ["Unit tests for all endpoints"]
        }
        
        try:
            # Create a test context
            context = AgentContext(
                project_requirements=test_reqs,
                completed_tasks=[],
                artifacts={},
                decisions=[],
                current_phase="testing"
            )
            
            # The tool function expects these parameters
            result = await test_tool.function(
                requirements_dict=test_reqs,
                project_path=".",
                context=context
            )
            print(f"  Tool execution successful:")
            print(f"  {result.split(chr(10))[0]}")  # First line of result
        except Exception as e:
            print(f"  Tool test error (expected in test environment): {str(e)[:100]}")

async def main():
    """Run all quality validation tests"""
    print("=" * 60)
    print("Testing Quality Guardian Enhancements")
    print("=" * 60)
    
    await test_requirement_validation()
    await test_docker_validation()
    await test_completion_report()
    await test_tool_integration()
    
    print("\n" + "=" * 60)
    print("‚úÖ All quality validation tests completed")
    print("=" * 60)
    
    print("\nüìä Summary of Quality Guardian Enhancements:")
    print("1. ‚úÖ Requirement validation with completion tracking")
    print("2. ‚úÖ Docker configuration validation")
    print("3. ‚úÖ Endpoint testing capabilities")
    print("4. ‚úÖ Completion metrics and reporting")
    print("5. ‚úÖ Critical issue identification")
    print("6. ‚úÖ Recommendations for improvement")
    
    print("\nüéØ Benefits for TaskManagerAPI:")
    print("- Complete requirement tracking (~40% ‚Üí measurable %)")
    print("- Automated validation of all deliverables")
    print("- Clear identification of missing components")
    print("- Actionable recommendations for completion")
    print("- Docker and endpoint validation before deployment")

if __name__ == "__main__":
    asyncio.run(main())